
@article{navarro_practical_nodate,
	title = {Practical, transparent operating system support for superpages},
	abstract = {Most general-purpose processors provide support for memory pages of large sizes, called superpages. Superpages enable each entry in the translation lookaside buffer (TLB) to map a large physical memory region into a virtual address space. This dramatically increases TLB coverage, reduces TLB misses, and promises performance improvements for many applications. However, supporting superpages poses several challenges to the operating system, in terms of superpage allocation and promotion tradeoffs, fragmentation control, etc. We analyze these issues, and propose the design of an effective superpage management system. We implement it in FreeBSD on the Alpha CPU, and evaluate it on real workloads and benchmarks. We obtain substantial performance beneﬁts, often exceeding 30\%; these beneﬁts are sustained even under stressful workload scenarios.},
	language = {en},
	author = {Navarro, Juan},
	file = {Navarro - Practical, transparent operating system support fo.pdf:/Users/akilan/Zotero/storage/9RBYAPGM/Navarro - Practical, transparent operating system support fo.pdf:application/pdf},
}

@inproceedings{panwar_hawkeye_2019,
	address = {Providence RI USA},
	title = {{HawkEye}: {Efficient} {Fine}-grained {OS} {Support} for {Huge} {Pages}},
	isbn = {978-1-4503-6240-5},
	shorttitle = {{HawkEye}},
	url = {https://dl.acm.org/doi/10.1145/3297858.3304064},
	doi = {10.1145/3297858.3304064},
	language = {en},
	urldate = {2024-05-27},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Panwar, Ashish and Bansal, Sorav and Gopinath, K.},
	month = apr,
	year = {2019},
	pages = {347--360},
	file = {Full Text PDF:/Users/akilan/Zotero/storage/VQLCKYCA/Panwar et al. - 2019 - HawkEye Efficient Fine-grained OS Support for Hug.pdf:application/pdf},
}

@article{basu_efficient_nodate,
	title = {Efficient {Virtual} {Memory} for {Big} {Memory} {Servers}},
	abstract = {Our analysis shows that many “big-memory” server workloads, such as databases, in-memory caches, and graph analytics, pay a high cost for page-based virtual memory. They consume as much as 10\% of execution cycles on TLB misses, even using large pages. On the other hand, we find that these workloads use read-write permission on most pages, are provisioned not to swap, and rarely benefit from the full flexibility of page-based virtual memory.},
	language = {en},
	author = {Basu, Arkaprava and Gandhi, Jayneel and Chang, Jichuan and Hill, Mark D and Swift, Michael M},
	file = {Basu et al. - Efficient Virtual Memory for Big Memory Servers.pdf:/Users/akilan/Zotero/storage/JJ5M79Q9/Basu et al. - Efficient Virtual Memory for Big Memory Servers.pdf:application/pdf},
}

@inproceedings{karakostas_redundant_2015,
	address = {Portland Oregon},
	title = {Redundant memory mappings for fast access to large memories},
	isbn = {978-1-4503-3402-0},
	url = {https://dl.acm.org/doi/10.1145/2749469.2749471},
	doi = {10.1145/2749469.2749471},
	abstract = {Page-based virtual memory improves programmer productivity, security, and memory utilization, but incurs performance overheads due to costly page table walks after TLB misses. This overhead can reach 50\% for modern workloads that access increasingly vast memory with stagnating TLB sizes. To reduce the overhead of virtual memory, this paper proposes Redundant Memory Mappings (RMM), which leverage ranges of pages and provides an efﬁcient, alternative representation of many virtual-to-physical mappings. We deﬁne a range be a subset of process’s pages that are virtually and physically contiguous. RMM translates each range with a single range table entry, enabling a modest number of entries to translate most of the process’s address space. RMM operates in parallel with standard paging and uses a software range table and hardware range TLB with arbitrarily large reach. We modify the operating system to automatically detect ranges and to increase their likelihood with eager page allocation. RMM is thus transparent to applications. We prototype RMM software in Linux and emulate the hardware. RMM performs substantially better than paging alone and huge pages, and improves a wider variety of workloads than direct segments (one range per program), reducing the overhead of virtual memory to less than 1\% on average.},
	language = {en},
	urldate = {2024-05-27},
	booktitle = {Proceedings of the 42nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Karakostas, Vasileios and Gandhi, Jayneel and Ayar, Furkan and Cristal, Adrián and Hill, Mark D. and McKinley, Kathryn S. and Nemirovsky, Mario and Swift, Michael M. and Ünsal, Osman},
	month = jun,
	year = {2015},
	pages = {66--78},
	file = {Karakostas et al. - 2015 - Redundant memory mappings for fast access to large.pdf:/Users/akilan/Zotero/storage/8JECES24/Karakostas et al. - 2015 - Redundant memory mappings for fast access to large.pdf:application/pdf},
}

@article{chen_flexpointer_2023,
	title = {{FlexPointer}: {Fast} {Address} {Translation} {Based} on {Range} {TLB} and {Tagged} {Pointers}},
	volume = {20},
	issn = {1544-3566, 1544-3973},
	shorttitle = {{FlexPointer}},
	url = {https://dl.acm.org/doi/10.1145/3579854},
	doi = {10.1145/3579854},
	abstract = {Page-based virtual memory relies on TLBs to accelerate the address translation. Nowadays, the gap between application workloads and the capacity of TLB continues to grow, bringing many costly TLB misses and making the TLB a performance bottleneck. Previous studies seek to narrow the gap by exploiting the contiguity of physical pages. One promising solution is to group pages that are both virtually and physically contiguous into a memory range. Recording range translations can greatly increase the TLB reach, but ranges are also hard to index because they have arbitrary bounds. The processor has to compare against all the boundaries to determine which range an address falls in, which restricts the usage of memory ranges.
            In this article, we propose a tagged-pointer-based scheme, FlexPointer, to solve the range indexing problem. The core insight of FlexPointer is that large memory objects are rare, so we can create memory ranges based on such objects and assign each of them a unique ID. With the range ID integrated into pointers, we can index the range TLB with IDs and greatly simplify its structure. Moreover, because the ID is stored in the unused bits of a pointer and is not manipulated by the address generation, we can shift the range lookup to an earlier stage, working in parallel with the address generation. According to our trace-based simulation results, FlexPointer can reduce nearly all the L1 TLB misses, and page walks for a variety of memory-intensive workloads. Compared with a 4K-page baseline system, FlexPointer shows a 14\% performance improvement on average and up to 2.8x speedup in the best case. For other workloads, FlexPointer shows no performance degradation.},
	language = {en},
	number = {2},
	urldate = {2024-05-27},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Chen, Dongwei and Tong, Dong and Yang, Chun and Yi, Jiangfang and Cheng, Xu},
	month = jun,
	year = {2023},
	pages = {1--24},
	file = {Full Text PDF:/Users/akilan/Zotero/storage/L9XGZDFK/Chen et al. - 2023 - FlexPointer Fast Address Translation Based on Ran.pdf:application/pdf},
}

@article{woodruff_cheri_2019,
	title = {{CHERI} {Concentrate}: {Practical} {Compressed} {Capabilities}},
	volume = {68},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9340, 1557-9956, 2326-3814},
	shorttitle = {{CHERI} {Concentrate}},
	url = {https://ieeexplore.ieee.org/document/8703061/},
	doi = {10.1109/TC.2019.2914037},
	abstract = {We present CHERI Concentrate, a new fat-pointer compression scheme applied to CHERI, the most developed capability-pointer system at present. Capability fat pointers are a primary candidate to enforce ﬁne-grained and non-bypassable security properties in future computer systems, although increased pointer size can severely affect performance. Thus, several proposals for capability compression have been suggested elsewhere that do not support legacy instruction sets, ignore features critical to the existing software base, and also introduce design inefﬁciencies to RISC-style processor pipelines. CHERI Concentrate improves on the state-of-the-art region-encoding efﬁciency, solves important pipeline problems, and eases semantic restrictions of compressed encoding, allowing it to protect a full legacy software stack. We present the ﬁrst quantitative analysis of compiled capability code, which we use to guide the design of the encoding format. We analyze and extend logic from the open-source CHERI prototype processor design on FPGA to demonstrate encoding efﬁciency, minimize delay of pointer arithmetic, and eliminate additional load-to-use delay. To verify correctness of our proposed high-performance logic, we present a HOL4 machine-checked proof of the decode and pointer-modify operations. Finally, we measure a 50\% to 75\% reduction in L2 misses for many compiled C-language benchmarks running under a commodity operating system using compressed 128-bit and 64-bit formats, demonstrating both compatibility with and increased performance over the uncompressed, 256-bit format.},
	language = {en},
	number = {10},
	urldate = {2024-05-27},
	journal = {IEEE Transactions on Computers},
	author = {Woodruff, Jonathan and Joannou, Alexandre and Xia, Hongyan and Fox, Anthony and Norton, Robert M. and Chisnall, David and Davis, Brooks and Gudka, Khilan and Filardo, Nathaniel W. and Markettos, A. Theodore and Roe, Michael and Neumann, Peter G. and Watson, Robert N. M. and Moore, Simon W.},
	month = oct,
	year = {2019},
	pages = {1455--1469},
	file = {Woodruff et al. - 2019 - CHERI Concentrate Practical Compressed Capabiliti.pdf:/Users/akilan/Zotero/storage/3SZUIWQ5/Woodruff et al. - 2019 - CHERI Concentrate Practical Compressed Capabiliti.pdf:application/pdf},
}
